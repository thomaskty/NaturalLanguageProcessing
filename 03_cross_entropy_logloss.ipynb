{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9fce87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Cross-Entropy Loss: 0.19763488164214868\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    y_true: list of true labels (0 or 1)\n",
    "    y_pred: list of predicted probabilities (between 0 and 1)\n",
    "    \"\"\"\n",
    "    assert len(y_true) == len(y_pred), \"Length mismatch\"\n",
    "    epsilon = 1e-15  # to avoid log(0)\n",
    "    n = len(y_true)\n",
    "    loss = 0.0\n",
    "\n",
    "    for i in range(n):\n",
    "        # Clipping predictions to avoid log(0)\n",
    "        p = min(max(y_pred[i], epsilon), 1 - epsilon)\n",
    "        loss += y_true[i] * math.log(p) + (1 - y_true[i]) * math.log(1 - p)\n",
    "\n",
    "    return -loss / n\n",
    "\n",
    "# Example usage\n",
    "y_true = [1, 0, 1, 1]\n",
    "y_pred = [0.9, 0.1, 0.8, 0.7]\n",
    "print(\"Binary Cross-Entropy Loss:\", binary_cross_entropy(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d12a15a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical Cross-Entropy Loss: 0.3635480396729776\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def categorical_cross_entropy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    y_true: list of lists (one-hot encoded true labels)\n",
    "    y_pred: list of lists (predicted probabilities for each class)\n",
    "    \"\"\"\n",
    "    epsilon = 1e-15\n",
    "    n_samples = len(y_true)\n",
    "    n_classes = len(y_true[0])\n",
    "    loss = 0.0\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        for j in range(n_classes):\n",
    "            p = min(max(y_pred[i][j], epsilon), 1 - epsilon)\n",
    "            loss += y_true[i][j] * math.log(p)\n",
    "\n",
    "    return -loss / n_samples\n",
    "\n",
    "# Example usage\n",
    "y_true = [\n",
    "    [1, 0, 0],  # class 0\n",
    "    [0, 1, 0],  # class 1\n",
    "    [0, 0, 1],  # class 2\n",
    "]\n",
    "\n",
    "y_pred = [\n",
    "    [0.7, 0.2, 0.1],\n",
    "    [0.1, 0.8, 0.1],\n",
    "    [0.2, 0.2, 0.6],\n",
    "]\n",
    "\n",
    "print(\"Categorical Cross-Entropy Loss:\", categorical_cross_entropy(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f37a1f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL Divergence D(P || Q): 0.04575811092471789\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def kl_divergence(P, Q):\n",
    "    \"\"\"\n",
    "    P: list of true distribution probabilities\n",
    "    Q: list of approximate distribution probabilities\n",
    "    \"\"\"\n",
    "    epsilon = 1e-15\n",
    "    assert len(P) == len(Q), \"Distributions must be of same length\"\n",
    "    divergence = 0.0\n",
    "\n",
    "    for p, q in zip(P, Q):\n",
    "        p = max(min(p, 1), 0)\n",
    "        q = max(min(q, 1), epsilon)  # avoid log(0)\n",
    "        if p > 0:\n",
    "            divergence += p * math.log(p / q)\n",
    "\n",
    "    return divergence\n",
    "\n",
    "# Example: P is the true distribution, Q is the approximation\n",
    "P = [0.1, 0.4, 0.5]\n",
    "Q = [0.2, 0.3, 0.5]\n",
    "\n",
    "print(\"KL Divergence D(P || Q):\", kl_divergence(P, Q))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281845db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
