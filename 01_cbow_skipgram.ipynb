{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d57c95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer # type: ignore[import]\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "501de0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the corpus\n",
    "corpus = ['The cat sat on the mat',\n",
    "          'The dog ran in the park',\n",
    "          'The bird sang in the tree']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2271914f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the corpus to a sequence of integers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e375c02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "sequences = tokenizer.texts_to_sequences(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "326949b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 3, 4, 5, 1, 6], [1, 7, 8, 2, 1, 9], [1, 10, 11, 2, 1, 12]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abcf898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences of words in the corpus: [[1, 3, 4, 5, 1, 6], [1, 7, 8, 2, 1, 9], [1, 10, 11, 2, 1, 12]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Sequences of words in the corpus:\", sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0336847e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Vocabulary size\n",
    "embedding_size = 10  # Size of the word embeddings\n",
    "window_size = 2  # Context window size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a59b4691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate context-target pairs\n",
    "contexts = []\n",
    "targets = []\n",
    "\n",
    "for sequence in sequences:\n",
    "    for i in range(window_size, len(sequence) - window_size):\n",
    "        context = sequence[i-window_size:i] + sequence[i+1:i+window_size+1]\n",
    "        target = sequence[i]\n",
    "        contexts.append(context)\n",
    "        targets.append(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0150e688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 6, 6)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequences), len(contexts), len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "babf465f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 3, 5, 1],\n",
       " [3, 4, 1, 6],\n",
       " [1, 7, 2, 1],\n",
       " [7, 8, 1, 9],\n",
       " [1, 10, 2, 1],\n",
       " [10, 11, 1, 12]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa8d13f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 5, 8, 2, 11, 2]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5d792b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(contexts)\n",
    "y = np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47044849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset and DataLoader\n",
    "class CBOWDataset(Dataset):\n",
    "    def __init__(self, contexts, targets):\n",
    "        self.contexts = torch.tensor(contexts, dtype=torch.long)\n",
    "        self.targets = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.contexts[idx], self.targets[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa433735",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CBOWDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de9ef79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CBOW model in PyTorch\n",
    "class CBOWModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super(CBOWModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.linear = nn.Linear(embedding_size, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embedded = self.embeddings(inputs)  # Get embeddings for the context words\n",
    "        # print(embedded.shape)\n",
    "        # Take the average of the context word embeddings\n",
    "        embedded_mean = embedded.mean(dim=1) # feature wise average\n",
    "        # print(embedded_mean.shape)\n",
    "\n",
    "        out = self.linear(embedded_mean)  # Feed to linear layer to get word probabilities\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50a3a643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.5277430216471353\n",
      "Epoch 10, Loss: 1.3286320368448894\n",
      "Epoch 20, Loss: 0.6813733577728271\n",
      "Epoch 30, Loss: 0.35370368758837384\n",
      "Epoch 40, Loss: 0.2033375451962153\n",
      "Epoch 50, Loss: 0.12779705474774042\n",
      "Epoch 60, Loss: 0.08518798028429349\n",
      "Epoch 70, Loss: 0.05930993209282557\n",
      "Epoch 80, Loss: 0.04336586408317089\n",
      "Epoch 90, Loss: 0.03280031184355418\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "model = CBOWModel(vocab_size=vocab_size, embedding_size=embedding_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for context, target in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(context)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {total_loss/len(dataloader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7629d592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[4, 6, 3, 0, 1, 0, 2, 5, 4, 1],\n",
       "         [5, 3, 7, 7, 1, 6, 4, 8, 8, 6],\n",
       "         [9, 2, 3, 1, 7, 1, 5, 4, 3, 8],\n",
       "         [9, 3, 4, 8, 2, 9, 9, 4, 9, 7]],\n",
       "\n",
       "        [[5, 7, 9, 1, 9, 7, 1, 2, 5, 3],\n",
       "         [3, 1, 8, 5, 6, 3, 3, 3, 3, 6],\n",
       "         [8, 3, 6, 5, 3, 9, 0, 2, 3, 7],\n",
       "         [8, 5, 7, 8, 1, 3, 2, 6, 6, 5]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_tensor = torch.randint(0, 10, (2,4,10))\n",
    "random_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b9a39dcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6.7500, 3.5000, 4.2500, 4.0000, 2.7500, 4.0000, 5.0000, 5.2500, 6.0000,\n",
       "         5.5000],\n",
       "        [6.0000, 4.0000, 7.5000, 4.7500, 4.7500, 5.5000, 1.5000, 3.2500, 4.2500,\n",
       "         5.2500]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert random_tensor type to floating point \n",
    "random_tensor_float = random_tensor.float()\n",
    "random_tensor_float.mean(dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d8f8df16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences of words in the corpus: [[1, 3, 4, 5, 1, 6], [1, 7, 8, 2, 1, 9], [1, 10, 11, 2, 1, 12]]\n",
      "Epoch 0, Loss: 2.6340874632199607\n",
      "Epoch 10, Loss: 1.6211028695106506\n",
      "Epoch 20, Loss: 1.4906717638174694\n",
      "Epoch 30, Loss: 1.4755606005589168\n",
      "Epoch 40, Loss: 1.4599988708893459\n",
      "Epoch 50, Loss: 1.4655363708734512\n",
      "Epoch 60, Loss: 1.4557153582572937\n",
      "Epoch 70, Loss: 1.4393378893534343\n",
      "Epoch 80, Loss: 1.461005449295044\n",
      "Epoch 90, Loss: 1.4402139087518055\n"
     ]
    }
   ],
   "source": [
    "# skipgram \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer # type: ignore[import]\n",
    "\n",
    "\n",
    "# Define the corpus\n",
    "corpus = ['The cat sat on the mat',\n",
    "          'The dog ran in the park',\n",
    "          'The bird sang in the tree']\n",
    "\n",
    "# Convert the corpus to a sequence of integers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import Counter\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "sequences = tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "print(\"Sequences of words in the corpus:\", sequences)\n",
    "\n",
    "# Parameters\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Vocabulary size\n",
    "embedding_size = 10  # Size of the word embeddings\n",
    "window_size = 2  # Context window size\n",
    "\n",
    "# Generate target-context pairs for Skip-gram\n",
    "def generate_skipgram_data(sequences, window_size):\n",
    "    contexts = []\n",
    "    targets = []\n",
    "    for sequence in sequences:\n",
    "        for i in range(window_size, len(sequence) - window_size):\n",
    "            target = sequence[i]\n",
    "            context_words = sequence[i - window_size:i] + sequence[i + 1:i + window_size + 1]\n",
    "            for context_word in context_words:\n",
    "                contexts.append(target)\n",
    "                targets.append(context_word)\n",
    "    return np.array(contexts), np.array(targets)\n",
    "\n",
    "X, y = generate_skipgram_data(sequences, window_size)\n",
    "\n",
    "# Create Dataset and DataLoader\n",
    "class SkipGramDataset(Dataset):\n",
    "    def __init__(self, contexts, targets):\n",
    "        self.contexts = torch.tensor(contexts, dtype=torch.long)\n",
    "        self.targets = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.contexts[idx], self.targets[idx]\n",
    "\n",
    "dataset = SkipGramDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Define Skip-gram model in PyTorch\n",
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.linear = nn.Linear(embedding_size, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embedded = self.embeddings(inputs)  # Get embedding for target word\n",
    "        out = self.linear(embedded)  # Predict context words from target word\n",
    "        return out\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = SkipGramModel(vocab_size=vocab_size, embedding_size=embedding_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for context, target in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(context)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {total_loss/len(dataloader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6960dc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
